{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined Kriging Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jan\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# These lines are necessary only if GemPy is not installed\n",
    "import sys, os\n",
    "sys.path.append(\"../../..\")\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ['MKL_THREADING_LAYER'] = 'GNU'\n",
    "\n",
    "# Importing GemPy, which takes really long\n",
    "import gempy as gp\n",
    "\n",
    "# Importing auxiliary libraries\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "from scipy import spatial\n",
    "from scipy import optimize\n",
    "from scipy import special\n",
    "import time\n",
    "from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Kriging(object):\n",
    "    # What do I want: \n",
    "    # passing: Data with properties, layer, geo_data --> meaning that I have to cut everything in here!\n",
    "    # --- this is basically done \n",
    "    # possibility to read in data - check gempy how to do best\n",
    "    # --- if data has own xyz coordinates I need to fit them to grid\n",
    "    # analyzing data part needs to be in here somewhere, have to check how \"OK\" that is,\n",
    "    # --- includes histogram and variogram building as well as fitting a function and converting to covariance function\n",
    "    # --- and if I can switch between covariance function (SK) and variogram function (OK, UK) in a reasonable manner\n",
    "    # Selection:\n",
    "    # SK, OK, UK\n",
    "    # --- SK needs covariance function instead of variogram function\n",
    "    # --- UK needs completely different setup as I need the coordinates of eacht point in Kriging matrices !!!\n",
    "    # cdist or my distance\n",
    "    # --- need to set up my distance calculation properly also maybe with several planes as suggested by Florian\n",
    "    # SGS or not?\n",
    "    def __init__(self, data, geomodel, grid, formation_number):\n",
    "        # here I want to put the basic variables and also the data analysis - or maybe even everything?!\n",
    "        #if '' in kwds:\n",
    "        #    self.set_nxny(kwds['nx'], kwds['ny'])\n",
    "        \n",
    "        self.geomodel = geomodel\n",
    "        self.grid = grid\n",
    "        self.formation_number = formation_number\n",
    "        self.data = data\n",
    "        \n",
    "        #PART 1: Initializing data and domain\n",
    "        # set domain data\n",
    "        self.grid_dataframe = self.init_domain()\n",
    "        # set property data (finally needs to be an 2d array with index in grid_dataframe and value)\n",
    "        self.property_data = self.match_data_positions()\n",
    "        # hier schnell nen workaround:\n",
    "        \n",
    "        #PART 2: Analyzing the data - mainly getting a variogram/covariance function \n",
    "        # best case, only do that if not done before, so only of nothing is specified in keywords\n",
    "        self.model = 'gaussian' # everything preset for now\n",
    "        self.range_ = 169\n",
    "        self.sill = 38\n",
    "        self.nugget = 5\n",
    "        \n",
    "        \n",
    "        #PART 3: Acutally performing Kriging \n",
    "        self.kriging_result, self.result_coord = self.sgs() \n",
    "        \n",
    "            \n",
    "        # get data - save it and tie it to corresponding grid points\n",
    "    \n",
    "    #def get_data():\n",
    "        # method to read data from given csv and create pandas dataframe\n",
    "        \n",
    "    def init_domain (self):\n",
    "        \"\"\"\n",
    "        Method to create a new pandas dataframe containing a grid for the SGS. Grid from complete geologic model is\n",
    "        reduced to a certain formation of interest. Thus as of now, Kriging is only possible within one layer of the\n",
    "        model. Allowing multiple layers or the whole model should not be too difficult if required.\n",
    "        Args:\n",
    "            geomodel (numpy.ndarray): lithological block model created with gempy\n",
    "            grid (gempy.data_management.GridClass): Grid created for geologic model\n",
    "            formation_number (int): Number of formation to perform CoKriging on\n",
    "        Returns:\n",
    "            pandas.dataframe: Dataframe with all relevant data for grid, meaning xyz of each grid point.\n",
    "        \"\"\"\n",
    "    \n",
    "        # convert lith block values to int, thats what Miguel suggested --> maybe a better solution required\n",
    "        geomodel_int = np.round(self.geomodel[0])\n",
    "    \n",
    "        # create the dataframe and populate with data\n",
    "        d = {'X': self.grid.values[:,0], 'Y': self.grid.values[:,1], 'Z': self.grid.values[:,2], 'lith': geomodel_int}\n",
    "        dataframe_aux = pd.DataFrame(data=d)\n",
    "\n",
    "        # cut down to wanted lithology and reset dataframne\n",
    "        grid_dataframe = dataframe_aux.loc[dataframe_aux['lith'] == self.formation_number]\n",
    "        grid_dataframe = grid_dataframe.reset_index() # reset indicies\n",
    "        del grid_dataframe['index'] # reset indices\n",
    "\n",
    "        return grid_dataframe\n",
    "    \n",
    "    def match_data_positions(self):\n",
    "        # more complex test run with the data I used before\n",
    "        data = self.data\n",
    "        prop_coord = data.as_matrix(('X','Y','Z'))\n",
    "        coord3d = self.grid_dataframe.values\n",
    "        coord3d = np.delete(coord3d, 3, 1)\n",
    "\n",
    "        # workaround to match data position to grid\n",
    "        coord3d_round = np.round(coord3d, 2) \n",
    "        prop_coord_round = np.round(prop_coord, 2)\n",
    "        \n",
    "        print(prop_coord_round[0])\n",
    "        print(coord3d_round[477])\n",
    "\n",
    "        # finding positions of prop_data in grid\n",
    "        prop_d = np.zeros(len(prop_coord))\n",
    "        \n",
    "        x = np.argwhere(np.all(coord3d_round == prop_coord_round[0], axis=1))\n",
    "        print(x)\n",
    "\n",
    "        for i in range (len(prop_coord)):\n",
    "            prop_d[i] = np.argwhere(np.all(coord3d_round == prop_coord_round[i], axis=1))[0][0]\n",
    "\n",
    "        values = data.as_matrix(('Property',))\n",
    "        prop_d = prop_d.reshape((200,1))\n",
    "\n",
    "        data_pos = np.hstack((prop_d, values))\n",
    "        data_pos = data_pos.transpose()\n",
    "        data_pos = data_pos.astype(int)\n",
    "        \n",
    "        return data_pos\n",
    "    \n",
    "            #def analyse_data():\n",
    "        # method for getting mean, std, etc. of the given dataset\n",
    "        \n",
    "    def precalculate_distances(self, prop_data, sgs_check, grid_coord):\n",
    "    \n",
    "        # order grid by indices given in prop_data[0] and sgs check\n",
    "        aux = np.append(prop_data[0], sgs_check)\n",
    "        grid_reordered = grid_coord[aux]\n",
    "\n",
    "        # perform cdist\n",
    "        dist_matrix = cdist(grid_reordered, grid_reordered)\n",
    "\n",
    "        return dist_matrix, grid_reordered\n",
    "    \n",
    "    def gaussian_variogram_model(self, d):\n",
    "        psill =self.sill-self.nugget\n",
    "        gamma = psill * (1. - np.exp(-d**2./(self.range_)**2.))+self.nugget\n",
    "        return gamma\n",
    "\n",
    "    def ordinary_kriging(self, a, b, prop):\n",
    "        \n",
    "        # empty matrix building\n",
    "        shape = len(a)\n",
    "        C = np.zeros((shape+1, shape+1))\n",
    "        c = np.zeros((shape+1))\n",
    "        w = np.zeros((shape+1))\n",
    "\n",
    "        # Faster matrix building approach, no loops\n",
    "        C[:shape, :shape] = self.gaussian_variogram_model(b)\n",
    "        c[:shape] = self.gaussian_variogram_model(a)\n",
    "\n",
    "        # matrix setup - compare pykrige, special for OK\n",
    "        np.fill_diagonal(C, 0) # is that OK?\n",
    "        C[shape, :] = 1.0\n",
    "        C[:, shape] = 1.0\n",
    "        C[shape, shape] = 0.0  \n",
    "        c[shape] = 1.0\n",
    "\n",
    "        # Solve Kriging equations\n",
    "        w = np.linalg.solve(C,c)\n",
    "\n",
    "        # SGS version - need to get mean and std\n",
    "        #result = np.random.normal(np.sum(w[:shape] * prop), scale=np.sqrt(w[shape]-gaussian_variogram_model(0)+np.sum(w[:shape]*c[:shape])))\n",
    "\n",
    "        # direct version, calculating result from weights.\n",
    "        result = np.sum(w[:shape] * prop)\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def universal_kriging(self, a, b, prop, coord, target_coord):\n",
    "\n",
    "        # empty matrix building\n",
    "        shape = len(a)\n",
    "        C = np.zeros((shape+1, shape+1))\n",
    "        c = np.zeros((shape+1))\n",
    "        w = np.zeros((shape+1))\n",
    "\n",
    "        # Faster matrix building approach, no loops\n",
    "        C[:shape, :shape] = self.gaussian_variogram_model(b)\n",
    "        c[:shape] = self.gaussian_variogram_model(a)\n",
    "\n",
    "        # matrix setup - compare pykrige\n",
    "        np.fill_diagonal(C, 0) # is that OK\n",
    "        C[shape, :] = 1.0\n",
    "        C[:, shape] = 1.0\n",
    "        C[shape, shape] = 0.0  \n",
    "        c[shape] = 1.0\n",
    "\n",
    "        # additional matrices for universal kriging, containing Coordinates and zeros\n",
    "        aux1 = np.vstack((coord, np.zeros((1, 3))))\n",
    "        aux2 = np.hstack((np.transpose(coord), np.zeros((3,4))))\n",
    "\n",
    "        # adding auxiliary matrices to the kriging matrices\n",
    "        C = np.hstack((C, aux1))\n",
    "        C = np.vstack((C, aux2))\n",
    "        c = np.hstack((c, target_coord))\n",
    "\n",
    "        # Solve Kriging equations\n",
    "        w = np.linalg.solve(C,c)\n",
    "\n",
    "        # SGS version - in UK case the scale (standard deviation) is not yet implemented/correct\n",
    "        #result = np.random.normal(np.sum(w[:shape] * data_v), scale=np.sqrt(w[shape]-gaussian_variogram_model(0)+np.sum(w[:shape]*c[:shape])))\n",
    "\n",
    "        # direct version, calculating result from weights.\n",
    "        result = np.sum(w[:shape] * prop)\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def get_distance_matrices(self, dist_matrix, n, prop_data, subcoord):\n",
    "        \"\"\"\n",
    "        Method to get closest points out of distance matrix calculated beforehand.\n",
    "        Args:\n",
    "            dist_matrix (np.array): matrix of distances between existing property data points\n",
    "            n(int): number of closest points used \n",
    "            prop_data(np.array)(n,2): array containing indexes of property data in grid and corresponding values\n",
    "        Returns\n",
    "            dist_close_all_to_all (n,n): matrix of distances between all selected closest points\n",
    "            dist_close_target_to_all (n,): distances between target point and selected closest points\n",
    "        \"\"\"\n",
    "        #index of target point, follows growth of dist matrix, as randomization is done before\n",
    "        target_point = len(dist_matrix)-1\n",
    "\n",
    "        # check for n closest points in matrix (minimum distance values) and sort the resulting index array\n",
    "        ind = np.argpartition(dist_matrix[target_point], n+1)[:n+1]\n",
    "        sort_ind = np.sort(ind)\n",
    "\n",
    "        # create new property array to match property data to correct indices\n",
    "        ind_aux_prop = sort_ind[:len(sort_ind)-1]\n",
    "        closest_prop_data = prop_data[1][ind_aux_prop]\n",
    "        \n",
    "        # only for UK also match coordinates\n",
    "        closest_coord_data = subcoord[ind_aux_prop]\n",
    "\n",
    "        # extract distances from target point (row of target index - now last row of matrix) without last entry (target)\n",
    "        dist_close_target_to_all = dist_matrix[target_point][sort_ind]\n",
    "        dist_close_target_to_all = dist_close_target_to_all[:len(dist_close_target_to_all)-1]\n",
    "\n",
    "        # extract distance each to each for those closest points, delete target point index row and column\n",
    "        dist_close_all_to_all = dist_matrix[np.ix_(sort_ind[:len(sort_ind)-1],sort_ind[:len(sort_ind)-1])]\n",
    "        \n",
    "        return dist_close_target_to_all, dist_close_all_to_all, closest_prop_data, closest_coord_data\n",
    "\n",
    "    def sgs(self):\n",
    "        \n",
    "        # copy data to leave original data unchanged\n",
    "        grid_coord = self.grid_dataframe.values[:, :-1]\n",
    "        prop_data = self.property_data\n",
    "    \n",
    "        # just for progress bar\n",
    "        runs = len(grid_coord)-len(prop_data[0])\n",
    "\n",
    "        # for timing purposes\n",
    "        time_prec = 0\n",
    "        time_sub = 0\n",
    "        time_dist = 0\n",
    "        time_krig = 0\n",
    "\n",
    "        # create array to go through SGS, only containing indices of grid points without data\n",
    "        sgs_check = np.arange(0,len(grid_coord))\n",
    "        sgs_check = np.delete(sgs_check, prop_data[0])\n",
    "    \n",
    "        # randomize it to predefine SGS way\n",
    "        np.random.shuffle(sgs_check)\n",
    "\n",
    "        # precalculate distances in matrix and sgs order, as well as coordinates in sgs order (for UK only)\n",
    "        t_pre1 = time.time()\n",
    "        dist_matrix_sgs_order, coord_sgs_order = self.precalculate_distances(prop_data, sgs_check, grid_coord)\n",
    "        t_pre2 = time.time()\n",
    "        print(\"distance precalculation:\", t_pre2-t_pre1)\n",
    "\n",
    "        # set initial length of property data frame\n",
    "        start = len(prop_data[0])\n",
    "\n",
    "        for i in range(0, len(sgs_check)):\n",
    "\n",
    "            # choose first point from sgs_check, as this is already randomized\n",
    "            target_point = sgs_check[i]\n",
    "            # for UK coordinate of target point\n",
    "            target_coord = grid_coord[sgs_check[i]]\n",
    "\n",
    "            t0 = time.time()\n",
    "            # extract submatrix required for distances, containing only grid points with property values\n",
    "            submatrix = dist_matrix_sgs_order[0:start+i+1,0:start+i+1] \n",
    "            \n",
    "            # for UK only to transfer coordinates:\n",
    "            subcoord = coord_sgs_order[0:start+i+1]\n",
    "\n",
    "            t1 = time.time()\n",
    "            time_sub = time_sub+(t1-t0)\n",
    "            # get closest distances\n",
    "            # a, b, prop = self.get_distance_matrices(submatrix, 50, prop_data)\n",
    "            # for UK version:\n",
    "            a, b, close_prop, close_coord = self.get_distance_matrices(submatrix, 50, prop_data, subcoord)\n",
    "\n",
    "            t2 = time.time()\n",
    "            time_dist = time_dist+(t2-t1)\n",
    "            # perform the Kriging interpolation on this point\n",
    "            # kriging_result = self.ordinary_kriging(a , b, close_prop)\n",
    "            \n",
    "            kriging_result = self.universal_kriging(a , b, close_prop, close_coord, target_coord)\n",
    "\n",
    "            t3 = time.time()\n",
    "            time_krig = time_krig+(t3-t2)\n",
    "\n",
    "            # add point to property data list\n",
    "            prop_data = np.hstack((prop_data, ([[target_point], [kriging_result]])))\n",
    "            #prop_data = np.round(prop_data) # just for now\n",
    "\n",
    "            #use of progress bar ...\n",
    "            #updt(runs,i+1)\n",
    "\n",
    "            \n",
    "        # print(prop_data.shape)   \n",
    "        # sort the results properly at the end - needs to be optimized for not rounding it \n",
    "        #prop_data = np.round(prop_data) # just for now\n",
    "        #prop_data = sorted(np.swapaxes(prop_data,0,1), key=lambda row: row[0])\n",
    "        #prop_data = np.vstack(prop_data)\n",
    "        #prop_data = np.swapaxes(prop_data,0,1)\n",
    "        print(prop_data.shape)\n",
    "        result_coord = np.swapaxes(coord_sgs_order, 0,1)\n",
    "        print(result_coord.shape)\n",
    "        \n",
    "\n",
    "        print(\"submatrix extraction:\", time_sub)\n",
    "        print(\"distance Matrices:\", time_dist)\n",
    "        print(\"kriging calculation:\", time_krig)\n",
    "\n",
    "        return prop_data, result_coord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nvariogram_fit = pd.read_csv(\"variogram_fit.csv\")\\nprop_coord = data.as_matrix((\\'X\\',\\'Y\\',\\'Z\\'))\\ncoord3d = domain.values\\ncoord3d = np.delete(coord3d, 3, 1)\\n\\n# workaround to match data position to grid\\ncoord3d_round = np.round(coord3d, 2) \\nprop_coord_round = np.round(prop_coord, 2)\\n\\n# finding positions of prop_data in grid\\nprop_d = np.zeros(len(prop_coord))\\n\\nfor i in range (len(prop_coord)):\\n    prop_d[i] = np.argwhere(np.all(coord3d_round == prop_coord_round[i], axis=1))[0][0]\\n    \\nvalues = data.as_matrix((\\'Property\\',))\\nprop_d = prop_d.reshape((200,1))\\n\\ndata_pos = np.hstack((prop_d, values))\\ndata_pos = data_pos.transpose()\\ndata_pos = data_pos.astype(int)\\n\\n# testing if this is correct !\\ncoord_check = coord3d[data_pos[0]]\\ncoord_ceck_aux = np.swapaxes(coord_check, 0,1) # for plotting later\\n\\nprint(prop_coord_round[0])\\nprint(coord3d_round[477])\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# more complex test run with the data I used before\n",
    "#domain = pd.read_csv(\"domain3d.csv\")\n",
    "data = pd.read_csv(\"data3d.csv\")\n",
    "'''\n",
    "variogram_fit = pd.read_csv(\"variogram_fit.csv\")\n",
    "prop_coord = data.as_matrix(('X','Y','Z'))\n",
    "coord3d = domain.values\n",
    "coord3d = np.delete(coord3d, 3, 1)\n",
    "\n",
    "# workaround to match data position to grid\n",
    "coord3d_round = np.round(coord3d, 2) \n",
    "prop_coord_round = np.round(prop_coord, 2)\n",
    "\n",
    "# finding positions of prop_data in grid\n",
    "prop_d = np.zeros(len(prop_coord))\n",
    "\n",
    "for i in range (len(prop_coord)):\n",
    "    prop_d[i] = np.argwhere(np.all(coord3d_round == prop_coord_round[i], axis=1))[0][0]\n",
    "    \n",
    "values = data.as_matrix(('Property',))\n",
    "prop_d = prop_d.reshape((200,1))\n",
    "\n",
    "data_pos = np.hstack((prop_d, values))\n",
    "data_pos = data_pos.transpose()\n",
    "data_pos = data_pos.astype(int)\n",
    "\n",
    "# testing if this is correct !\n",
    "coord_check = coord3d[data_pos[0]]\n",
    "coord_ceck_aux = np.swapaxes(coord_check, 0,1) # for plotting later\n",
    "\n",
    "print(prop_coord_round[0])\n",
    "print(coord3d_round[477])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jan\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:6211: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort)\n"
     ]
    }
   ],
   "source": [
    "# Importing the data from CSV-files and setting extent and resolution, example without faults\n",
    "geo_data = gp.create_data([0,3000,0,200,0,2000],resolution=[120,4,80], \n",
    "                         path_o = \"C:/Users/Jan/gempy/notebooks/input_data/tut_chapter3/tutorial_ch3_foliations\", # importing orientation (foliation) data\n",
    "                         path_i = \"C:/Users/Jan/gempy/notebooks/input_data/tut_chapter3/tutorial_ch3_interfaces\") # importing point-positional interface data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling theano function...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.tensor.blas): We did not find a dynamic library in the library_dir of the library we use for blas. If you use ATLAS, make sure to compile it with dynamics library.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compilation Done!\n",
      "Level of Optimization:  fast_compile\n",
      "Device:  cpu\n",
      "Precision:  float32\n",
      "Number of faults:  0\n"
     ]
    }
   ],
   "source": [
    "interp_data = gp.InterpolatorData(geo_data, u_grade=[1], output='geology', compile_theano=True, theano_optimizer='fast_compile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jan\\Anaconda3\\lib\\site-packages\\theano\\tensor\\subtensor.py:2320: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  out[0][inputs[2:]] = inputs[1]\n"
     ]
    }
   ],
   "source": [
    "lith_block, fault_block = gp.compute_model(interp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jan\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:81: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[387.5    0.83 686.18]\n",
      "[387.5    0.83 686.18]\n",
      "[]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-ebdc3cf06900>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKriging\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlith_block\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgeo_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformation_number\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-17-2b28ae608946>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, geomodel, grid, formation_number)\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrid_dataframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_domain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;31m# set property data (finally needs to be an 2d array with index in grid_dataframe and value)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproperty_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatch_data_positions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m         \u001b[1;31m# hier schnell nen workaround:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-2b28ae608946>\u001b[0m in \u001b[0;36mmatch_data_positions\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprop_coord\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m             \u001b[0mprop_d\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcoord3d_round\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mprop_coord_round\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Property'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "test = Kriging(data, lith_block, geo_data.grid, formation_number=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.kriging_result.shape\n",
    "coord3d_aux = test.grid_dataframe.values\n",
    "coord3d_aux = np.delete(coord3d_aux, 3, 1)\n",
    "coord3d_aux = np.swapaxes(coord3d_aux, 0,1) # for plotting later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "\n",
    "# define the colormap\n",
    "cmap = plt.cm.PuBu_r\n",
    "# extract all colors\n",
    "cmaplist = [cmap(i) for i in range(cmap.N)]\n",
    "# force the first color entry to be grey\n",
    "cmaplist[0] = (.5,.5,.5,1.0)\n",
    "# create the new map\n",
    "cmap = cmap.from_list('Custom cmap', cmaplist, cmap.N)\n",
    "\n",
    "# define the bins and normalize\n",
    "bounds = np.linspace(0,40,8)\n",
    "norm = mpl.colors.BoundaryNorm(bounds, cmap.N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14, 12))\n",
    "ax = Axes3D(fig)\n",
    "\n",
    "ax.axes.set_zlim3d(0,2000)\n",
    "ax.axes.set_xlim3d(0,3000)\n",
    "ax.axes.set_ylim3d(0,200)\n",
    "a = ax.scatter3D(xs=test.result_coord[0],ys=test.result_coord[1],zs=test.result_coord[2], c=test.kriging_result[1], s=20, marker=',', cmap=cmap, norm=norm)\n",
    "#b = ax.scatter3D(xs=data.X,ys=data.Y,zs=data.Z, c=data.Property, s=50, marker='o', cmap=cmap, norm=norm, linewidths=1, edgecolors='black')\n",
    "# c = ax.scatter3D(xs=coord_ceck_aux[0],ys=coord_ceck_aux[1],zs=coord_ceck_aux[2], c=data_pos[1], s=50, marker='o', cmap=cmap, norm=norm, linewidths=1, edgecolors='black')\n",
    "\n",
    "\n",
    "fig.colorbar(a, orientation='horizontal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# double check\n",
    "input_result = test.kriging_result[:,:200]\n",
    "input_coord = test.result_coord[:,:200]\n",
    "print(np.swapaxes(input_coord,0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Found my error !!!!\n",
    "\n",
    "fig = plt.figure(figsize=(14, 12))\n",
    "ax = Axes3D(fig)\n",
    "\n",
    "ax.axes.set_zlim3d(0,2000)\n",
    "ax.axes.set_xlim3d(0,3000)\n",
    "ax.axes.set_ylim3d(0,200)\n",
    "a = ax.scatter3D(xs=input_coord[0],ys=input_coord[1],zs=input_coord[2], c=input_result[1], s=50, marker='o', cmap=cmap, norm=norm, linewidths=1, edgecolors='black')\n",
    "#b = ax.scatter3D(xs=coord_ceck_aux[0],ys=coord_ceck_aux[1],zs=coord_ceck_aux[2], s=50, marker='o', cmap=cmap, norm=norm, linewidths=1, edgecolors='black')\n",
    "\n",
    "fig.colorbar(a, orientation='horizontal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
