{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined Kriging Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jan\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# These lines are necessary only if GemPy is not installed\n",
    "import sys, os\n",
    "sys.path.append(\"../../..\")\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ['MKL_THREADING_LAYER'] = 'GNU'\n",
    "\n",
    "# Importing GemPy, which takes really long\n",
    "import gempy as gp\n",
    "\n",
    "# Importing auxiliary libraries\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "from scipy import spatial\n",
    "from scipy import optimize\n",
    "from scipy import special\n",
    "from skimage import measure\n",
    "from scipy import sparse\n",
    "from scipy.sparse.linalg import splu\n",
    "import time\n",
    "from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Kriging(object):\n",
    "    # What do I want: \n",
    "    # passing: Data with properties, layer, geo_data --> meaning that I have to cut everything in here! - check\n",
    "    # --- this is basically done \n",
    "    # possibility to read in data - check gempy how to do best\n",
    "    # --- if data has own xyz coordinates I need to fit them to grid\n",
    "    # analyzing data part needs to be in here somewhere, have to check how \"OK\" that is,\n",
    "    # --- includes histogram and variogram building as well as fitting a function and converting to covariance function\n",
    "    # --- and if I can switch between covariance function (SK) and variogram function (OK, UK) in a reasonable manner\n",
    "    # Selection:\n",
    "    # SK, OK, UK\n",
    "    # --- SK needs covariance function instead of variogram function - check\n",
    "    # --- UK needs completely different setup as I need the coordinates of eacht point in Kriging matrices - check\n",
    "    # cdist or my distance\n",
    "    # --- need to set up my distance calculation properly also maybe with several planes as suggested by Florian\n",
    "    # SGS or not?\n",
    "    def __init__(self, data, geomodel, geodata, formation_number, kriging_type=None, distance_type=None, variogram_model=None, faultmodel=None):\n",
    "        # here I want to put the basic variables and also the data analysis - or maybe even everything?!\n",
    "        \n",
    "        self.geomodel = geomodel\n",
    "        self.grid = geodata.grid\n",
    "        self.formation_number = formation_number\n",
    "        self.data = data\n",
    "        self.extent = geodata.extent\n",
    "        self.resolution = geodata.resolution\n",
    "        \n",
    "        # control parameters\n",
    "        self.neg_var_count = 0\n",
    "        self.control_coord = np.zeros((1,3))\n",
    "        \n",
    "        # Basic input data parameters for SK \n",
    "        self.inp_mean = np.mean(data['Property'])\n",
    "        self.inp_var = np.var(data['Property'])\n",
    "        self.inp_std = np.sqrt(self.inp_var)\n",
    "        \n",
    "        # Set default Kriging type to OK\n",
    "        if kriging_type is None:\n",
    "            kriging_type = 'OK'\n",
    "        self.kriging_type = kriging_type\n",
    "        \n",
    "        # Set default distance type to euclidian\n",
    "        if distance_type is None:\n",
    "            distance_type = 'euclidian'\n",
    "        self.distance_type = distance_type\n",
    "        \n",
    "        # Set default distance type to euclidian\n",
    "        if variogram_model is None:\n",
    "            variogram_model = 'exponential'\n",
    "            self.variogram_model = variogram_model\n",
    "        \n",
    "        # figure out fault shit\n",
    "        if faultmodel is not None:\n",
    "            self.faultmodel = faultmodel\n",
    "        else:\n",
    "            self.faultmodel = np.zeros(len(self.geomodel))\n",
    "        \n",
    "        print(self.faultmodel)\n",
    "        print(self.faultmodel.shape)\n",
    "        \n",
    "        t_init1 = time.time()\n",
    "        #PART 1: Initializing data and domain\n",
    "        # set domain data\n",
    "        self.grid_dataframe = self.init_domain()\n",
    "        # set property data (finally needs to be an 2d array with index in grid_dataframe and value)\n",
    "        self.property_data = self.match_data_positions() \n",
    "        t_init2 = time.time()\n",
    "        print(\"initializing:\", t_init2-t_init1)\n",
    "        \n",
    "        #PART 2: Analyzing the data - mainly getting a variogram/covariance function \n",
    "        # best case, only do that if not done before, so only of nothing is specified in keywords\n",
    "        # preset everything for now, maybe make this optional later, either pass sill, range, model, and nugget\n",
    "        # or make scipy bestfit\n",
    "        self.range_ = 150\n",
    "        self.sill = 40\n",
    "        self.nugget = 1\n",
    "        \n",
    "        #PART 3: Acutally performing Kriging \n",
    "        self.kriging_result, self.result_coord = self.sgs() \n",
    "        \n",
    "    \n",
    "    #def get_data():\n",
    "        # method to read data from given csv and create pandas dataframe\n",
    "        # question if this is really necessary, or if passing a pandas dataframe is sufficient\n",
    "        \n",
    "    def init_domain (self):\n",
    "        \"\"\"\n",
    "        Method to create a new pandas dataframe containing a grid for the SGS. Grid from complete geologic model is\n",
    "        reduced to a certain formation of interest. Thus as of now, Kriging is only possible within one layer of the\n",
    "        model. Allowing multiple layers or the whole model should not be too difficult if required.\n",
    "        Args:\n",
    "            geomodel (numpy.ndarray): lithological block model created with gempy\n",
    "            grid (gempy.data_management.GridClass): Grid created for geologic model\n",
    "            formation_number (int): Number of formation to perform CoKriging on\n",
    "        Returns:\n",
    "            pandas.dataframe: Dataframe with all relevant data for grid, meaning xyz of each grid point.\n",
    "        \"\"\"\n",
    "    \n",
    "        # convert lith block values to int, thats what Miguel suggested --> maybe a better solution required\n",
    "        geomodel_int = np.round(self.geomodel[0])\n",
    "        scalar_field_value = self.geomodel[1]\n",
    "    \n",
    "        # create the dataframe and populate with data\n",
    "        d = {'X': self.grid.values[:,0], 'Y': self.grid.values[:,1], 'Z': self.grid.values[:,2], 'lith': geomodel_int, 'scalar': scalar_field_value}\n",
    "        dataframe_aux = pd.DataFrame(data=d)\n",
    "\n",
    "        # cut down to wanted lithology and reset dataframne\n",
    "        grid_dataframe = dataframe_aux.loc[dataframe_aux['lith'] == self.formation_number]\n",
    "        grid_dataframe = grid_dataframe.reset_index() # reset indicies\n",
    "        del grid_dataframe['index'] # reset indices\n",
    "\n",
    "        return grid_dataframe\n",
    "    \n",
    "    def match_data_positions(self):\n",
    "        '''\n",
    "        Method to match XYZ coordinates of measured property data to grid points, as distances are computed \n",
    "        grid points. Right now also points outside the specified domain are projected onto the grid. Maybe add\n",
    "        if clause for greater distances.\n",
    "        Args:\n",
    "            data (pandas.dataframe): dataframe containing xyz coordinates and property values\n",
    "            grid_dataframe: containing xyz of grid coordinates\n",
    "        Returns\n",
    "            data_pos(np.array)(2,n) = array containing grid index [0] and corresponding property value [1]\n",
    "            '''\n",
    "        # extracting required data\n",
    "        prop_coord = self.data.values[:,:3]\n",
    "        values = self.data.values[:,3]\n",
    "        coord3d = self.grid_dataframe.values[:,:3]\n",
    "        \n",
    "        # empty aray for results\n",
    "        data_pos = np.zeros((len(values), 2))\n",
    "       \n",
    "        # finding closest grid point for each data value\n",
    "        for i in range (len(self.data)):\n",
    "            closest = np.argmin(cdist(prop_coord[i].reshape(1,3), coord3d))\n",
    "            data_pos[i][0]= closest\n",
    "            data_pos[i][1]= values[i]\n",
    "        \n",
    "        # workaround for rounding - hast to be optimized\n",
    "        data_pos = data_pos.astype(int)\n",
    "        data_pos = np.swapaxes(data_pos,0,1)\n",
    "        \n",
    "        return data_pos\n",
    "    \n",
    "            #def analyse_data():\n",
    "        # method for getting mean, std, etc. of the given dataset\n",
    "        \n",
    "    def precalculate_distances(self, prop_data, sgs_check, grid_coord):\n",
    "        \n",
    "        # order grid by indices given in prop_data[0] and sgs check\n",
    "        aux = np.append(prop_data[0], sgs_check)\n",
    "        grid_reordered = grid_coord[aux]\n",
    "        \n",
    "        if self.distance_type == 'euclidian':\n",
    "            # perform cdist - easy and fast for straight distances\n",
    "            dist_matrix = cdist(grid_reordered, grid_reordered)\n",
    "        elif self.distance_type == 'deformed':\n",
    "            dist_matrix = self.dist_all_to_all(grid_reordered)\n",
    "            \n",
    "        return dist_matrix, grid_reordered\n",
    "    \n",
    "    def dist_all_to_all(self, grid_reordered):\n",
    "        # 1: Calculte central average plane within domain between top and bottom border\n",
    "        med_ver, med_sim, grad_plane = self.create_central_plane()\n",
    "        \n",
    "        # 1.5 Qick plotting option of refernece plane for crosscheck\n",
    "        # fig = plt.figure(figsize=(16,10))\n",
    "        # ax = fig.add_subplot(1, 1, 1, projection='3d')\n",
    "        # a = ax.plot_trisurf(med_ver[:,0], med_ver[:,1], med_ver[:,2], triangles=med_sim)\n",
    "        \n",
    "        # 2: project each point in domain on this plane (by closest point) and save this as reference point\n",
    "        ref, perp = self.projection_of_each_point(med_ver, grad_plane, grid_reordered)\n",
    "        # 3: Calculate distances on reference plane by heat method\n",
    "        dist_clean = self.proj_surface_dist_each_to_each(med_ver, med_sim)\n",
    "        # 4: Combine results to final distance matrix, here i should set stretch factors\n",
    "        dist_matrix = self.distances_grid(ref, perp, dist_clean)\n",
    "\n",
    "        return dist_matrix\n",
    "    \n",
    "    def create_central_plane(self):\n",
    "        # find highest and lowest scalar field value (top and bot layer)\n",
    "        grad_top = np.max(self.grid_dataframe.scalar)\n",
    "        grad_bot = np.min(self.grid_dataframe.scalar)\n",
    "        # do precalculations, mesh through basic point (only once)\n",
    "        a = self.geomodel[1].reshape(self.resolution)\n",
    "        grad = (grad_top+grad_bot)/2\n",
    "\n",
    "        vertices, simplices, normals, values = measure.marching_cubes_lewiner(\n",
    "                a,\n",
    "                grad,\n",
    "                step_size=1,\n",
    "                spacing=((self.extent[1]/self.resolution[0]),(self.extent[3]/self.resolution[1]),(self.extent[5]/self.resolution[2])))\n",
    "\n",
    "        return vertices, simplices, grad\n",
    "    \n",
    "    def projection_of_each_point(self, ver, plane_grad, grid_reordered):\n",
    "    \n",
    "        grid = grid_reordered[:,:3]\n",
    "        # create matrix from grid data, as well as empty array for results\n",
    "        # grad_check = self.grid_dataframe.as_matrix(('scalar',))[:,0] # old version\n",
    "        grad_check = grid_reordered[:,4] # new version, seems to be equivalent\n",
    "        grad_check = grad_check > plane_grad\n",
    "        ref = np.zeros(len(grid))\n",
    "        perp = np.zeros(len(grid))\n",
    "\n",
    "        # loop through grid to refernce each point to closest point on reference plane by index\n",
    "        for i in range(len(grid)):\n",
    "            ref[i] = cdist(grid[i].reshape(1,3), ver).argmin()\n",
    "            # get the cdistance to closest point and save it\n",
    "            perp[i] = cdist(grid[i].reshape(1,3), ver).min()\n",
    "\n",
    "        # reshape perp to make values either negative or positive (depending on scalar field value)\n",
    "        # there has to be an easier way to do it with the a mask\n",
    "        for i in range(len(perp)):\n",
    "            if grad_check[i]==True:\n",
    "                perp[i]=perp[i]*(-1)\n",
    "\n",
    "        return ref, perp\n",
    "    \n",
    "    def proj_surface_dist_each_to_each(self, med_ver, med_sim):\n",
    "    \n",
    "        # precomputing \n",
    "        compute_distance = GeodesicDistanceComputation(med_ver, med_sim)\n",
    "\n",
    "        # create empty matrix\n",
    "        dist = np.zeros([len(med_ver), len(med_ver)])\n",
    "\n",
    "        for i in range(len(med_ver)):\n",
    "            # distance calculation\n",
    "            dist[i] = compute_distance(i)\n",
    "        \n",
    "        # operation to take average of dist, as heat method is not exact\n",
    "        dist_clean = (dist+dist.T)/2\n",
    "\n",
    "        return dist_clean\n",
    "    \n",
    "    def distances_grid(self, ref, perp, dist_clean):\n",
    "        # stretch factor test, does not work with variogram model like this\n",
    "        dist_clean = dist_clean/10000\n",
    "    \n",
    "        dist_matrix = np.zeros([len(ref), len(ref)])\n",
    "        ref = ref.astype(int)\n",
    "\n",
    "        for i in range(len(ref)):\n",
    "            for j in range(len(ref)):\n",
    "                dist_matrix[i][j]=dist_clean[ref[i]][ref[j]]+abs(perp[i]-perp[j])\n",
    "\n",
    "        # algorithm is not optimal for short distances, thus putting them to minium possible cdist value for resolution\n",
    "        # seems very reasonable, here that is 25\n",
    "        # dist_matrix[dist_matrix < 25] = 25\n",
    "        np.fill_diagonal(dist_matrix, 0)\n",
    "\n",
    "        return dist_matrix\n",
    "    \n",
    "    def gaussian_variogram_model(self, d):\n",
    "        psill =self.sill-self.nugget\n",
    "        gamma = psill * (1. - np.exp(-d**2./(self.range_)**2.))+self.nugget\n",
    "        return gamma\n",
    "    \n",
    "    def gaussian_covariance_model(self, d):\n",
    "        # variance normed to values\n",
    "        self.sill = self.inp_var/self.inp_mean\n",
    "        gamma = self.sill * (np.exp(-d**2./(self.range_)**2.))\n",
    "        return gamma\n",
    "    \n",
    "    def exponential_variogram_model(self, d):\n",
    "        psill =self.sill-self.nugget\n",
    "        gamma = psill * (1. - np.exp(-(np.absolute(d)/(self.range_))))+self.nugget\n",
    "        return gamma\n",
    "\n",
    "    def simple_kriging(self, a, b, prop):\n",
    "        \n",
    "        # !!! SK still needs work: result only works with diagonal (nugget effect???) fill of 10 and covariance \n",
    "        # function seems more than shady ... but choosing kriging type works for all three options now\n",
    "        # Also SGS does only work with scale = 0 so does not really wor at all :/\n",
    "\n",
    "        # empty matrix buildung\n",
    "        shape = len(a)\n",
    "        C = np.zeros((shape, shape))\n",
    "        c = np.zeros((shape))\n",
    "        w = np.zeros((shape))\n",
    "\n",
    "        # Faster matrix building approach, no loops\n",
    "        C[:shape, :shape] = self.gaussian_covariance_model(b)\n",
    "        c[:shape] = self.gaussian_covariance_model(a)\n",
    "\n",
    "        # nugget effect\n",
    "        np.fill_diagonal(C, 10)\n",
    "\n",
    "        w = np.linalg.solve(C,c)\n",
    "\n",
    "        # SGS version - taking result from normal distribution with kriging mean an standard deviation\n",
    "        #result = np.random.normal(inp_mean + np.sum(w * (prop-inp_mean)), scale = 0) #scale=np.sqrt(variance-np.sum(w*c)))\n",
    "        # if I use other scale it gets wild\n",
    "\n",
    "        # direct version, calculating result from weights. Need to be normed to one\n",
    "        result = self.inp_mean + np.sum(w * (prop-self.inp_mean))\n",
    "\n",
    "        return result\n",
    "\n",
    "    def ordinary_kriging(self, a, b, prop, target):\n",
    "        \n",
    "        # empty matrix building\n",
    "        shape = len(a)\n",
    "        C = np.zeros((shape+1, shape+1))\n",
    "        c = np.zeros((shape+1))\n",
    "        w = np.zeros((shape+1))\n",
    "\n",
    "        if self.variogram_model == 'gaussian':\n",
    "            # Faster matrix building approach, no loops\n",
    "            C[:shape, :shape] = self.gaussian_variogram_model(b)\n",
    "            c[:shape] = self.gaussian_variogram_model(a)\n",
    "        elif self.variogram_model == 'exponential':\n",
    "            # Faster matrix building approach, no loops\n",
    "            C[:shape, :shape] = self.exponential_variogram_model(b)\n",
    "            c[:shape] = self.exponential_variogram_model(a)\n",
    "        elif self.variogram_model == 'spherical':\n",
    "            print('not yet implemented')\n",
    "        else:\n",
    "            print('Variogram model not understood')\n",
    "\n",
    "        # matrix setup - compare pykrige, special for OK\n",
    "        np.fill_diagonal(C, 0) # is that OK?\n",
    "        C[shape, :] = 1.0\n",
    "        C[:, shape] = 1.0\n",
    "        C[shape, shape] = 0.0  \n",
    "        c[shape] = 1.0\n",
    "            \n",
    "        # Solve Kriging equations\n",
    "        w = np.linalg.solve(C,c)\n",
    "\n",
    "        # correcting for negative weights:\n",
    "        #w_cor = w[:shape]\n",
    "        #w_cor[w_cor < 0] = 0\n",
    "        #w_cor = (w_cor/np.sum(w_cor))\n",
    "        \n",
    "        # SGS version - need to get mean and std\n",
    "        #pred_var = w[shape]+np.sum(w_cor*c[:shape]) # for corrected weights\n",
    "        pred_var = w[shape]+np.sum(w[:shape]*c[:shape])\n",
    "        \n",
    "        # count for negative variances for security, seems to be solved by using exponential\n",
    "        if pred_var <= 0:\n",
    "            # result = np.sum(w_cor * prop)\n",
    "            # control mechanism\n",
    "            self.neg_var_count = self.neg_var_count + 1\n",
    "            # self.control_coord = np.vstack((self.control_coord, target[:3]))\n",
    "        \n",
    "        #result = np.random.normal(np.sum(w_cor  * prop), scale=np.sqrt(pred_var)) # code for corrected weights\n",
    "        \n",
    "        result = np.random.normal(np.sum(w[:shape]  * prop), scale=np.sqrt(pred_var))\n",
    "        \n",
    "        # direct version, calculating result from weights.\n",
    "        # result = np.sum(w_cor * prop)\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def universal_kriging(self, a, b, prop, coord, target_coord):\n",
    "\n",
    "        # empty matrix building\n",
    "        shape = len(a)\n",
    "        C = np.zeros((shape+1, shape+1))\n",
    "        c = np.zeros((shape+1))\n",
    "        w = np.zeros((shape+1))\n",
    "\n",
    "        # Faster matrix building approach, no loops\n",
    "        C[:shape, :shape] = self.gaussian_variogram_model(b)\n",
    "        c[:shape] = self.gaussian_variogram_model(a)\n",
    "\n",
    "        # matrix setup - compare pykrige\n",
    "        np.fill_diagonal(C, 0) # is that OK\n",
    "        C[shape, :] = 1.0\n",
    "        C[:, shape] = 1.0\n",
    "        C[shape, shape] = 0.0  \n",
    "        c[shape] = 1.0\n",
    "\n",
    "        # additional matrices for universal kriging, containing Coordinates and zeros\n",
    "        aux1 = np.vstack((coord[:,:3], np.zeros((1, 3))))\n",
    "        aux2 = np.hstack((np.transpose(coord[:,:3]), np.zeros((3,4))))\n",
    "\n",
    "        # adding auxiliary matrices to the kriging matrices\n",
    "        C = np.hstack((C, aux1))\n",
    "        C = np.vstack((C, aux2))\n",
    "        c = np.hstack((c, target_coord[:3]))\n",
    "\n",
    "        # Solve Kriging equations\n",
    "        w = np.linalg.solve(C,c)\n",
    "\n",
    "        # SGS version - in UK case the scale (standard deviation) is not yet implemented/correct\n",
    "        #result = np.random.normal(np.sum(w[:shape] * data_v), scale=np.sqrt(w[shape]-gaussian_variogram_model(0)+np.sum(w[:shape]*c[:shape])))\n",
    "\n",
    "        # direct version, calculating result from weights.\n",
    "        result = np.sum(w[:shape] * prop)\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def get_distance_matrices(self, dist_matrix, n, prop_data, subcoord):\n",
    "        \"\"\"\n",
    "        Method to get closest points out of distance matrix calculated beforehand.\n",
    "        Args:\n",
    "            dist_matrix (np.array): matrix of distances between existing property data points\n",
    "            n(int): number of closest points used \n",
    "            prop_data(np.array)(n,2): array containing indexes of property data in grid and corresponding values\n",
    "        Returns\n",
    "            dist_close_all_to_all (n,n): matrix of distances between all selected closest points\n",
    "            dist_close_target_to_all (n,): distances between target point and selected closest points\n",
    "        \"\"\"\n",
    "        # condition if less data points are available # does work for anything but exactly one input point - SGS field\n",
    "        if len(dist_matrix)-1 < n:\n",
    "            n = len(dist_matrix)-1\n",
    "        \n",
    "        #index of target point, follows growth of dist matrix, as randomization is done before\n",
    "        target_point = len(dist_matrix)-1\n",
    "\n",
    "        # check for n closest points in matrix (minimum distance values) and sort the resulting index array\n",
    "        # ind = np.argpartition(dist_matrix[target_point], n+1)[:n+1] # this was first (wrong?) way\n",
    "        ind = np.argpartition(dist_matrix[target_point], n)[:n]\n",
    "        sort_ind = np.sort(ind)\n",
    "\n",
    "        # create new property array to match property data to correct indices\n",
    "        ind_aux_prop = sort_ind[:len(sort_ind)-1]\n",
    "        closest_prop_data = prop_data[1][ind_aux_prop]\n",
    "        \n",
    "        # only for UK also match coordinates\n",
    "        closest_coord_data = subcoord[ind_aux_prop]\n",
    "\n",
    "        # extract distances from target point (row of target index - now last row of matrix) without last entry (target)\n",
    "        dist_close_target_to_all = dist_matrix[target_point][sort_ind]\n",
    "        dist_close_target_to_all = dist_close_target_to_all[:len(dist_close_target_to_all)-1]\n",
    "\n",
    "        # extract distance each to each for those closest points, delete target point index row and column\n",
    "        dist_close_all_to_all = dist_matrix[np.ix_(sort_ind[:len(sort_ind)-1],sort_ind[:len(sort_ind)-1])]\n",
    "        \n",
    "        return dist_close_target_to_all, dist_close_all_to_all, closest_prop_data, closest_coord_data\n",
    "\n",
    "    def sgs(self):\n",
    "        \n",
    "        # copy data to leave original data unchanged\n",
    "        # grid_coord = self.grid_dataframe.values[:, :-1]\n",
    "        grid_coord = self.grid_dataframe.values[:, :]\n",
    "        prop_data = self.property_data\n",
    "    \n",
    "        # just for progress bar\n",
    "        runs = len(grid_coord)-len(prop_data[0])\n",
    "\n",
    "        # for timing purposes\n",
    "        time_prec = 0\n",
    "        time_sub = 0\n",
    "        time_dist = 0\n",
    "        time_krig = 0\n",
    "\n",
    "        # create array to go through SGS, only containing indices of grid points without data\n",
    "        sgs_check = np.arange(0,len(grid_coord))\n",
    "        sgs_check = np.delete(sgs_check, prop_data[0])\n",
    "    \n",
    "        # randomize it to predefine SGS way\n",
    "        np.random.shuffle(sgs_check)\n",
    "\n",
    "        # precalculate distances in matrix and sgs order, as well as coordinates in sgs order (for UK only)\n",
    "        t_pre1 = time.time()\n",
    "        dist_matrix_sgs_order, coord_sgs_order = self.precalculate_distances(prop_data, sgs_check, grid_coord)\n",
    "        t_pre2 = time.time()\n",
    "        print(\"distance precalculation:\", t_pre2-t_pre1)\n",
    "        \n",
    "        #self.plot_distances(dist_matrix_sgs_order, coord_sgs_order)\n",
    "\n",
    "        # set initial length of property data frame\n",
    "        start = len(prop_data[0])\n",
    "\n",
    "        for i in range(0, len(sgs_check)):\n",
    "\n",
    "            # choose first point from sgs_check, as this is already randomized\n",
    "            target_point = sgs_check[i]\n",
    "            # for UK coordinate of target point\n",
    "            target_coord = grid_coord[sgs_check[i]]\n",
    "\n",
    "            t0 = time.time()\n",
    "            # extract submatrix required for distances, containing only grid points with property values\n",
    "            submatrix = dist_matrix_sgs_order[0:start+i+1,0:start+i+1] \n",
    "            \n",
    "            # for UK only to transfer coordinates:\n",
    "            subcoord = coord_sgs_order[0:start+i+1]\n",
    "\n",
    "            t1 = time.time()\n",
    "            time_sub = time_sub+(t1-t0)\n",
    "            # get closest distances\n",
    "            # a, b, prop = self.get_distance_matrices(submatrix, 50, prop_data)\n",
    "            # for UK version:\n",
    "            a, b, close_prop, close_coord = self.get_distance_matrices(submatrix, 20, prop_data, subcoord)\n",
    "                \n",
    "            t2 = time.time()\n",
    "            time_dist = time_dist+(t2-t1)\n",
    "            # perform the Kriging interpolation on this point\n",
    "            \n",
    "            if self.kriging_type == 'OK':\n",
    "                kriging_result = self.ordinary_kriging(a , b, close_prop, target_coord)\n",
    "            elif self.kriging_type == 'UK':\n",
    "                kriging_result = self.universal_kriging(a , b, close_prop, close_coord, target_coord)\n",
    "            elif self.kriging_type == 'SK':\n",
    "                kriging_result = self.simple_kriging(a, b, close_prop)\n",
    "            else:\n",
    "                print('ERORR - Kriging Type not understood')\n",
    "\n",
    "            t3 = time.time()\n",
    "            time_krig = time_krig+(t3-t2)\n",
    "\n",
    "            # add point to property data list\n",
    "            prop_data = np.hstack((prop_data, ([[target_point], [kriging_result]])))\n",
    "\n",
    "            #use of progress bar ...\n",
    "            #updt(runs,i+1)\n",
    "\n",
    "            \n",
    "        # print(prop_data.shape)   \n",
    "        # sort the results properly at the end - needs to be optimized for not rounding it \n",
    "        #prop_data = np.round(prop_data) # just for now\n",
    "        #prop_data = sorted(np.swapaxes(prop_data,0,1), key=lambda row: row[0])\n",
    "        #prop_data = np.vstack(prop_data)\n",
    "        #prop_data = np.swapaxes(prop_data,0,1)\n",
    "        result_coord = np.swapaxes(coord_sgs_order, 0,1)\n",
    "    \n",
    "        print(\"submatrix extraction:\", time_sub)\n",
    "        print(\"distance Matrices:\", time_dist)\n",
    "        print(\"kriging calculation:\", time_krig)\n",
    "        \n",
    "        print(\"Negative Variances:\", self.neg_var_count)\n",
    "        print(\"Out of:\", len(coord_sgs_order))\n",
    "\n",
    "        return prop_data, result_coord\n",
    "    \n",
    "    \n",
    "    # different plotting tools for visualization\n",
    "    def plot_distances(self, distances, coord):\n",
    "        # plotting distances with respect to one arbitrarily choosen reference point (here 3000)\n",
    "        # shoud insted make it a random one in raneg of domain later\n",
    "        fig = plt.figure(figsize=(16,10))\n",
    "        ax = fig.add_subplot(1, 1, 1, projection='3d')\n",
    "        a = ax.scatter3D(xs=coord[:,0], ys=coord[:,1], zs=coord[:,2], s=5, c=distances[3000], cmap ='nipy_spectral')\n",
    "        b = ax.scatter3D(xs=coord[3000,0], ys=coord[3000,1], zs=coord[3000,2], s=100, c='black')\n",
    "        fig.colorbar(a, orientation='horizontal')\n",
    "        \n",
    "    #def plot_variogram():\n",
    "    \n",
    "def veclen(vectors):\n",
    "    \"\"\" return L2 norm (vector length) along the last axis, for example to compute the length of an array of vectors \"\"\"\n",
    "    return np.sqrt(np.sum(vectors**2, axis=-1))\n",
    "\n",
    "def normalized(vectors):\n",
    "    \"\"\" normalize array of vectors along the last axis \"\"\"\n",
    "    return vectors / veclen(vectors)[..., np.newaxis]\n",
    "\n",
    "\n",
    "def compute_mesh_laplacian(verts, tris):\n",
    "    \"\"\"\n",
    "    computes a sparse matrix representing the discretized laplace-beltrami operator of the mesh\n",
    "    given by n vertex positions (\"verts\") and a m triangles (\"tris\") \n",
    "    \n",
    "    verts: (n, 3) array (float)\n",
    "    tris: (m, 3) array (int) - indices into the verts array\n",
    "    computes the conformal weights (\"cotangent weights\") for the mesh, ie:\n",
    "    w_ij = - .5 * (cot \\alpha + cot \\beta)\n",
    "    See:\n",
    "        Olga Sorkine, \"Laplacian Mesh Processing\"\n",
    "        and for theoretical comparison of different discretizations, see \n",
    "        Max Wardetzky et al., \"Discrete Laplace operators: No free lunch\"\n",
    "    returns matrix L that computes the laplacian coordinates, e.g. L * x = delta\n",
    "    \"\"\"\n",
    "    n = len(verts)\n",
    "    W_ij = np.empty(0)\n",
    "    I = np.empty(0, np.int32)\n",
    "    J = np.empty(0, np.int32)\n",
    "    for i1, i2, i3 in [(0, 1, 2), (1, 2, 0), (2, 0, 1)]: # for edge i2 --> i3 facing vertex i1\n",
    "        vi1 = tris[:,i1] # vertex index of i1\n",
    "        vi2 = tris[:,i2]\n",
    "        vi3 = tris[:,i3]\n",
    "        # vertex vi1 faces the edge between vi2--vi3\n",
    "        # compute the angle at v1\n",
    "        # add cotangent angle at v1 to opposite edge v2--v3\n",
    "        # the cotangent weights are symmetric\n",
    "        u = verts[vi2] - verts[vi1]\n",
    "        v = verts[vi3] - verts[vi1]\n",
    "        cotan = (u * v).sum(axis=1) / veclen(np.cross(u, v))\n",
    "        W_ij = np.append(W_ij, 0.5 * cotan)\n",
    "        I = np.append(I, vi2)\n",
    "        J = np.append(J, vi3)\n",
    "        W_ij = np.append(W_ij, 0.5 * cotan)\n",
    "        I = np.append(I, vi3)\n",
    "        J = np.append(J, vi2)\n",
    "    L = sparse.csr_matrix((W_ij, (I, J)), shape=(n, n)) \n",
    "    \n",
    "    # compute diagonal entries\n",
    "    L = L - sparse.spdiags(L * np.ones(n), 0, n, n)\n",
    "    L = L.tocsr()\n",
    "    # area matrix\n",
    "    e1 = verts[tris[:,1]] - verts[tris[:,0]]\n",
    "    e2 = verts[tris[:,2]] - verts[tris[:,0]]\n",
    "    n = np.cross(e1, e2)\n",
    "    triangle_area = .5 * veclen(n)\n",
    "    # compute per-vertex area\n",
    "    vertex_area = np.zeros(len(verts))\n",
    "    ta3 = triangle_area / 3\n",
    "    for i in range(tris.shape[1]): # Jan: changed xrange to range\n",
    "        bc = np.bincount(tris[:,i].astype(int), ta3)\n",
    "        vertex_area[:len(bc)] += bc  \n",
    "    VA = sparse.spdiags(vertex_area, 0, len(verts), len(verts))\n",
    "    \n",
    "    return L, VA\n",
    "\n",
    "\n",
    "class GeodesicDistanceComputation(object):\n",
    "    \"\"\" \n",
    "    Computation of geodesic distances on triangle meshes using the heat method from the impressive paper\n",
    "        Geodesics in Heat: A New Approach to Computing Distance Based on Heat Flow\n",
    "        Keenan Crane, Clarisse Weischedel, Max Wardetzky\n",
    "        ACM Transactions on Graphics (SIGGRAPH 2013)\n",
    "    Example usage:\n",
    "        >>> compute_distance = GeodesicDistanceComputation(vertices, triangles)\n",
    "        >>> distance_of_each_vertex_to_vertex_0 = compute_distance(0)\n",
    "    \"\"\"\n",
    "    def __init__(self, verts, tris, m=10.0):\n",
    "        self._verts = verts\n",
    "        self._tris = tris\n",
    "        # precompute some stuff needed later on\n",
    "        e01 = verts[tris[:,1]] - verts[tris[:,0]]\n",
    "        e12 = verts[tris[:,2]] - verts[tris[:,1]]\n",
    "        e20 = verts[tris[:,0]] - verts[tris[:,2]]\n",
    "        self._triangle_area = .5 * veclen(np.cross(e01, e12))\n",
    "        unit_normal = normalized(np.cross(normalized(e01), normalized(e12)))\n",
    "        self._unit_normal_cross_e01 = np.cross(unit_normal, e01)\n",
    "        self._unit_normal_cross_e12 = np.cross(unit_normal, e12)\n",
    "        self._unit_normal_cross_e20 = np.cross(unit_normal, e20)\n",
    "        # parameters for heat method\n",
    "        h = np.mean(list(map(veclen, [e01, e12, e20]))) # Jan: converted to list\n",
    "        \n",
    "        # Jan: m is constant optimized at 1, here 10 is used\n",
    "        # Jan: h is mean distance between nodes/length of edges\n",
    "        t = m * h ** 2\n",
    "        \n",
    "        # pre-factorize poisson systems\n",
    "        Lc, A = compute_mesh_laplacian(verts, tris) \n",
    "        self._factored_AtLc = splu((A - t * Lc).tocsc()).solve\n",
    "        self._factored_L = splu(Lc.tocsc()).solve\n",
    "        \n",
    "    def __call__(self, idx):\n",
    "        \"\"\" \n",
    "        computes geodesic distances to all vertices in the mesh\n",
    "        idx can be either an integer (single vertex index) or a list of vertex indices\n",
    "        or an array of bools of length n (with n the number of vertices in the mesh) \n",
    "        \"\"\"\n",
    "        u0 = np.zeros(len(self._verts))\n",
    "        u0[idx] = 1.0\n",
    "        # heat method, step 1\n",
    "        u = self._factored_AtLc(u0).ravel()\n",
    "        # heat method step 2\n",
    "        grad_u = 1 / (2 * self._triangle_area)[:,np.newaxis] * (\n",
    "              self._unit_normal_cross_e01 * u[self._tris[:,2]][:,np.newaxis]\n",
    "            + self._unit_normal_cross_e12 * u[self._tris[:,0]][:,np.newaxis]\n",
    "            + self._unit_normal_cross_e20 * u[self._tris[:,1]][:,np.newaxis]\n",
    "        )\n",
    "        X = - grad_u / veclen(grad_u)[:,np.newaxis]\n",
    "        # heat method step 3\n",
    "        div_Xs = np.zeros(len(self._verts))\n",
    "        for i1, i2, i3 in [(0, 1, 2), (1, 2, 0), (2, 0, 1)]: # for edge i2 --> i3 facing vertex i1\n",
    "            vi1, vi2, vi3 = self._tris[:,i1], self._tris[:,i2], self._tris[:,i3]\n",
    "            e1 = self._verts[vi2] - self._verts[vi1]\n",
    "            e2 = self._verts[vi3] - self._verts[vi1]\n",
    "            e_opp = self._verts[vi3] - self._verts[vi2]\n",
    "            cot1 = 1 / np.tan(np.arccos( \n",
    "                (normalized(-e2) * normalized(-e_opp)).sum(axis=1)))\n",
    "            cot2 = 1 / np.tan(np.arccos(\n",
    "                (normalized(-e1) * normalized( e_opp)).sum(axis=1)))\n",
    "            div_Xs += np.bincount(\n",
    "                vi1.astype(int), \n",
    "        0.5 * (cot1 * (e1 * X).sum(axis=1) + cot2 * (e2 * X).sum(axis=1)), \n",
    "        minlength=len(self._verts))\n",
    "        phi = self._factored_L(div_Xs).ravel()\n",
    "        phi -= phi.min()\n",
    "        return phi  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more complex test run with the data I used before\n",
    "# domain = pd.read_csv(\"domain3d.csv\")\n",
    "data = pd.read_csv(\"data3d.csv\")\n",
    "variogram_fit = pd.read_csv(\"variogram_fit.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jan\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:6211: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort)\n"
     ]
    }
   ],
   "source": [
    "# Importing the data from CSV-files and setting extent and resolution, example without faults\n",
    "geo_data = gp.create_data([0,3000,0,200,0,2000],resolution=[120,4,80], \n",
    "                         path_o = \"C:/Users/Jan/gempy/notebooks/input_data/tut_chapter3/tutorial_ch3_foliations\", # importing orientation (foliation) data\n",
    "                         path_i = \"C:/Users/Jan/gempy/notebooks/input_data/tut_chapter3/tutorial_ch3_interfaces\") # importing point-positional interface data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling theano function...\n",
      "Compilation Done!\n",
      "Level of Optimization:  fast_compile\n",
      "Device:  cpu\n",
      "Precision:  float32\n",
      "Number of faults:  0\n"
     ]
    }
   ],
   "source": [
    "interp_data = gp.InterpolatorData(geo_data, u_grade=[1], output='geology', compile_theano=True, theano_optimizer='fast_compile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jan\\Anaconda3\\lib\\site-packages\\theano\\tensor\\subtensor.py:2320: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  out[0][inputs[2:]] = inputs[1]\n"
     ]
    }
   ],
   "source": [
    "lith_block, fault_block = gp.compute_model(interp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_single = data.iloc[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0.]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Kriging' object has no attribute 'fauktmodel'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-e134fc3a4f09>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKriging\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_single\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlith_block\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgeo_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformation_number\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkriging_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'OK'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdistance_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'deformed'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-2-f08e729e4ac9>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, geomodel, geodata, formation_number, kriging_type, distance_type, variogram_model, faultmodel)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfaultmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfauktmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[0mt_init1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Kriging' object has no attribute 'fauktmodel'"
     ]
    }
   ],
   "source": [
    "test = Kriging(test_single, lith_block, geo_data, formation_number=3, kriging_type='OK', distance_type='deformed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.kriging_result.shape\n",
    "coord3d_aux = test.grid_dataframe.values\n",
    "coord3d_aux = np.delete(coord3d_aux, 3, 1)\n",
    "coord3d_aux = np.swapaxes(coord3d_aux, 0,1) # for plotting later\n",
    "\n",
    "contr_coord = test.control_coord\n",
    "contr_coord = np.delete(contr_coord, (0), axis=0)\n",
    "contr_coord = np.swapaxes(contr_coord, 0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "\n",
    "# define the colormap\n",
    "cmap = plt.cm.PuBu_r\n",
    "# extract all colors\n",
    "cmaplist = [cmap(i) for i in range(cmap.N)]\n",
    "# force the first color entry to be grey\n",
    "cmaplist[0] = (.5,.5,.5,1.0)\n",
    "# create the new map\n",
    "cmap = cmap.from_list('Custom cmap', cmaplist, cmap.N)\n",
    "\n",
    "# define the bins and normalize\n",
    "bounds = np.linspace(0,40,8)\n",
    "norm = mpl.colors.BoundaryNorm(bounds, cmap.N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14, 12))\n",
    "ax = Axes3D(fig)\n",
    "\n",
    "ax.axes.set_zlim3d(0,2000)\n",
    "ax.axes.set_xlim3d(0,3000)\n",
    "ax.axes.set_ylim3d(0,200)\n",
    "a = ax.scatter3D(xs=test.result_coord[0],ys=test.result_coord[1],zs=test.result_coord[2], c=test.kriging_result[1], s=20, marker=',', cmap=cmap, norm=norm, alpha=1)\n",
    "#b = ax.scatter3D(xs=test.result_coord[0][:200],ys=test.result_coord[1][:200],zs=test.result_coord[2][:200], c=test.kriging_result[1][:200], s=20 ,marker=',', cmap=cmap, norm=norm, linewidths=1, edgecolors='black')\n",
    "c = ax.scatter3D(xs=data.X,ys=data.Y,zs=data.Z, c=data.Property, s=50, marker='o', cmap=cmap, norm=norm, linewidths=1, edgecolors='black')\n",
    "#d = ax.scatter3D(xs=contr_coord[0],ys=contr_coord[1],zs=contr_coord[2], c='black', s=5, marker='o')\n",
    "\n",
    "fig.colorbar(a, orientation='horizontal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test mit Störung\n",
    "# Importing the data from CSV-files and setting extent and resolution\n",
    "geo_data1 = gp.create_data([0,1000,0,20,0,1000],[100,2,100], \n",
    "      path_o = \"C:/Users/Jan/gempy/notebooks/prototype_notebooks/JanThesis/Jan_thesis_model5_orientations_slice.csv\",\n",
    "      path_i = \"C:/Users/Jan/gempy/notebooks/prototype_notebooks/JanThesis/Jan_thesis_model5_interfaces_slice.csv\") \n",
    "\n",
    "# Assigning series to formations as well as their order (timewise)\n",
    "gp.set_series(geo_data1, {\"Fault_Series\":'fault', \n",
    "                         \"Strat_Series\": ('rock2','rock1')},\n",
    "                       order_series = [\"Fault_Series\", 'Strat_Series'],\n",
    "                       order_formations=['fault', \n",
    "                                         'rock2','rock1'\n",
    "                                         ], verbose=0) \n",
    "\n",
    "interp_data1 = gp.InterpolatorData(geo_data1,\n",
    "                                  output='geology', compile_theano=True,\n",
    "                                  theano_optimizer='fast_compile',\n",
    "                                  verbose=[])\n",
    "\n",
    "lith_block1, fault_block1 = gp.compute_model(interp_data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "gp.plotting.plot_data(geo_data1, direction='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "gp.plotting.plot_section(geo_data1, lith_block1[0], cell_number=1,\n",
    "                         direction='y', plot_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = Kriging(test_single, lith_block1, geo_data1, formation_number=3, kriging_type='OK', distance_type='deformed', faultmodel=fault_block1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14, 12))\n",
    "ax = Axes3D(fig)\n",
    "\n",
    "ax.axes.set_zlim3d(0,1000)\n",
    "ax.axes.set_xlim3d(0,1000)\n",
    "ax.axes.set_ylim3d(0,20)\n",
    "a = ax.scatter3D(xs=test1.result_coord[0],ys=test1.result_coord[1],zs=test1.result_coord[2], c=test1.kriging_result[1], s=20, marker=',', cmap=cmap, norm=norm, alpha=1)\n",
    "#b = ax.scatter3D(xs=test.result_coord[0][:200],ys=test.result_coord[1][:200],zs=test.result_coord[2][:200], c=test.kriging_result[1][:200], s=20 ,marker=',', cmap=cmap, norm=norm, linewidths=1, edgecolors='black')\n",
    "#c = ax.scatter3D(xs=data.X,ys=data.Y,zs=data.Z, c=data.Property, s=50, marker='o', cmap=cmap, norm=norm, linewidths=1, edgecolors='black')\n",
    "\n",
    "fig.colorbar(a, orientation='horizontal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
